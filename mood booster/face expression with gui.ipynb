{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c124cbc70ad4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mLabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'FRAME 2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mButton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Go to frame 3'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mButton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'close'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'run' is not defined"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "\n",
    "\n",
    "def raise_frame(frame):\n",
    "    frame.tkraise()\n",
    "\n",
    "root = Tk()\n",
    "\n",
    "f1 = Frame(root)\n",
    "f2 = Frame(root)\n",
    "\n",
    "for frame in (f1, f2):\n",
    "    frame.grid(row=0, column=0, sticky='news')\n",
    "\n",
    "Button(f1, text='Go to frame 2', command=lambda:raise_frame(f2)).pack()\n",
    "Label(f1, text='FRAME 1').pack()\n",
    "\n",
    "Label(f2, text='FRAME 2').pack()\n",
    "Button(f2, text='Go to frame 3', command=run).pack()\n",
    "\n",
    "Button(f2, text='close',command=root.destroy).pack()\n",
    "\n",
    "raise_frame(f1)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    import numpy as np \n",
    "    import cv2\n",
    "    import keras\n",
    "    from keras.models import load_model\n",
    "    from statistics import mode\n",
    "\n",
    "    # parameters for loading data and images\n",
    "    emotion_model_path = 'emotion_model.hdf5'\n",
    "   \n",
    "    emotion_labels = {0:'angry',1:'disgust',2:'fear',3:'happy',4:'sad',5:'surprise',6:'neutral'}\n",
    "\n",
    "\n",
    "\n",
    "    # hyper-parameters for bounding boxes shape\n",
    "    frame_window = 10\n",
    "    emotion_offsets = (20, 40)\n",
    "    # loading models\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    emotion_classifier = load_model(emotion_model_path)\n",
    "    print(emotion_classifier)\n",
    "    # getting input model shapes for inference\n",
    "    emotion_target_size = emotion_classifier.input_shape[1:3]\n",
    "\n",
    "    # starting lists for calculating modes\n",
    "    emotion_window = []\n",
    "\n",
    "\n",
    "    def preprocess_input(x, v2=True):\n",
    "        x = x.astype('float32')\n",
    "        x = x / 255.0\n",
    "        if v2:\n",
    "            x = x - 0.5\n",
    "            x = x * 2.0\n",
    "        return x\n",
    "\n",
    "    def draw_bounding_box(face_coordinates, image_array, color):\n",
    "        x, y, w, h = face_coordinates\n",
    "        cv2.rectangle(image_array, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "    def draw_text(coordinates, image_array, text, color, x_offset=0, y_offset=0,\n",
    "                                                    font_scale=2, thickness=2):\n",
    "        x, y = coordinates[:2]\n",
    "        cv2.putText(image_array, text, (x + x_offset, y + y_offset),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    font_scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "    def apply_offsets(face_coordinates, offsets):\n",
    "        x, y, width, height = face_coordinates\n",
    "        x_off, y_off = offsets\n",
    "        return (x - x_off, x + width + x_off, y - y_off, y + height + y_off)\n",
    "\n",
    "\n",
    "    cv2.namedWindow('window_frame')\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    while cap.isOpened(): # True:\n",
    "        ret, bgr_image = cap.read()\n",
    "\n",
    "        #bgr_image = video_capture.read()[1]\n",
    "\n",
    "        gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n",
    "        rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5,minSize=(30, 30))#, flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "        for face_coordinates in faces:\n",
    "\n",
    "            x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n",
    "\n",
    "            gray_face = gray_image[y1:y2, x1:x2]\n",
    "            try:\n",
    "                gray_face = cv2.resize(gray_face, (emotion_target_size))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            gray_face = preprocess_input(gray_face, True)\n",
    "            gray_face = np.expand_dims(gray_face, 0)\n",
    "            gray_face = np.expand_dims(gray_face, -1)\n",
    "            emotion_prediction = emotion_classifier.predict(gray_face)\n",
    "            emotion_probability = np.max(emotion_prediction)\n",
    "            emotion_label_arg = np.argmax(emotion_prediction)\n",
    "            emotion_text = emotion_labels[emotion_label_arg]\n",
    "            emotion_window.append(emotion_text)\n",
    "\n",
    "            if len(emotion_window) > frame_window:\n",
    "                emotion_window.pop(0)\n",
    "            try:\n",
    "                emotion_mode = mode(emotion_window)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            if emotion_text == 'angry':\n",
    "                color = emotion_probability * np.asarray((255, 0, 0))\n",
    "            elif emotion_text == 'sad':\n",
    "                color = emotion_probability * np.asarray((0, 0, 255))\n",
    "            elif emotion_text == 'happy':\n",
    "                color = emotion_probability * np.asarray((255, 255, 0))\n",
    "            elif emotion_text == 'surprise':\n",
    "                color = emotion_probability * np.asarray((0, 255, 255))\n",
    "            else:\n",
    "                color = emotion_probability * np.asarray((0, 255, 0))\n",
    "\n",
    "            color = color.astype(int)\n",
    "            color = color.tolist()\n",
    "\n",
    "            draw_bounding_box(face_coordinates, rgb_image, color)\n",
    "            draw_text(face_coordinates, rgb_image, emotion_mode,\n",
    "                      color, 0, -45, 1, 1)\n",
    "\n",
    "        bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imshow('window_frame', bgr_image)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9baefcffc1e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mLabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'FRAME 2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mButton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Go to frame 3'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[0mButton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Go to frame 2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mButton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'close'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'run' is not defined"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "    \n",
    "import PIL\n",
    "from PIL import Image,ImageTk\n",
    "import pytesseract\n",
    "import cv2\n",
    "from tkinter import *\n",
    "width, height = 800, 600\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "def show_frame():\n",
    "    _, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    cv2image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGBA)\n",
    "    img = PIL.Image.fromarray(cv2image)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "    lmain.imgtk = imgtk\n",
    "    lmain.configure(image=imgtk)\n",
    "    lmain.after(10, show_frame)\n",
    "    \n",
    "def raise_frame(frame):\n",
    "    frame.tkraise()\n",
    "\n",
    "root = Tk()\n",
    "\n",
    "f1 = Frame(root)\n",
    "f2 = Frame(root)\n",
    "\n",
    "for frame in (f1, f2):\n",
    "    frame.grid(row=0, column=0, sticky='news')\n",
    "\n",
    "lmain = Label(f2)\n",
    "lmain.pack()\n",
    "\n",
    "Button(f1, text='Go to frame 2', command=lambda:raise_frame(f2)).pack()\n",
    "Label(f1, text='FRAME 1').pack()\n",
    "\n",
    "Label(f2, text='FRAME 2').pack()\n",
    "Button(f2, text='Go to frame 3', command=run).pack()\n",
    "Button(f2, text='Go to frame 2',command=show_frame).pack()\n",
    "Button(f2, text='close',command=root.destroy).pack()\n",
    "\n",
    "raise_frame(f1)\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raise_frame(frame):\n",
    "    frame.tkraise()\n",
    "\n",
    "def preprocess_input(x, v2=True):\n",
    "    x = x.astype('float32')\n",
    "    x = x / 255.0\n",
    "    if v2:\n",
    "        x = x - 0.5\n",
    "        x = x * 2.0\n",
    "    return x\n",
    "\n",
    "def draw_bounding_box(face_coordinates, image_array, color):\n",
    "    x, y, w, h = face_coordinates\n",
    "    cv2.rectangle(image_array, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "def draw_text(coordinates, image_array, text, color, x_offset=0, y_offset=0,font_scale=2, thickness=2):\n",
    "    x, y = coordinates[:2]\n",
    "    cv2.putText(image_array, text, (x + x_offset, y + y_offset),cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                font_scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "def apply_offsets(face_coordinates, offsets):\n",
    "    x, y, width, height = face_coordinates\n",
    "    x_off, y_off = offsets\n",
    "    return (x - x_off, x + width + x_off, y - y_off, y + height + y_off)\n",
    "\n",
    "    \n",
    "def show_frame():\n",
    "   \n",
    "    # hyper-parameters for bounding boxes shape\n",
    "    frame_window = 10\n",
    "    emotion_offsets = (20, 40)\n",
    "    # loading models\n",
    "\n",
    "     # getting input model shapes for inference\n",
    "    emotion_target_size = emotion_classifier.input_shape[1:3]\n",
    "\n",
    "    # starting lists for calculating modes\n",
    "    emotion_window = []\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    cv2.putText(frame,'OpenCV',(50,50), cv2.FONT_HERSHEY_SIMPLEX, 4,(255,255,255),2,cv2.LINE_AA)\n",
    "    cv2image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGBA)\n",
    "    img = PIL.Image.fromarray(cv2image)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "    \n",
    "    lmain.imgtk = imgtk\n",
    "    lmain.configure(image=imgtk)\n",
    "    lmain.after(10, show_frame)\n",
    "    \n",
    "\n",
    "import PIL\n",
    "from PIL import Image,ImageTk\n",
    "import pytesseract\n",
    "import cv2\n",
    "from tkinter import *\n",
    "import numpy as np \n",
    "import cv2\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from statistics import mode\n",
    "\n",
    "    # parameters for loading data and images\n",
    "emotion_model_path = 'emotion_model.hdf5'\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "emotion_classifier = load_model(emotion_model_path)\n",
    "   \n",
    "emotion_labels = {0:'angry',1:'disgust',2:'fear',3:'happy',4:'sad',5:'surprise',6:'neutral'}\n",
    "width, height = 800, 600\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "root = Tk()\n",
    "f1 = Frame(root)\n",
    "f2 = Frame(root)\n",
    "for frame in (f1, f2):\n",
    "    frame.grid(row=0, column=0, sticky='news')\n",
    "\n",
    "lmain = Label(f1)\n",
    "lmain.pack()\n",
    "Button(f1, text='webcam',command=show_frame).pack()\n",
    "Button(f1, text='close',command=root.destroy).pack()\n",
    "Button(f1, text='page 2', command=lambda:raise_frame(f2)).pack()\n",
    "Label(f1, text='FRAME 1').pack()\n",
    "\n",
    "Button(f2, text='close',command=root.destroy).pack()\n",
    "Button(f2, text='page 2', command=lambda:raise_frame(f1)).pack()\n",
    "Label(f2, text='FRAME 1').pack()\n",
    "\n",
    "raise_frame(f1)\n",
    "root.mainloop()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raise_frame(frame):\n",
    "    frame.tkraise()\n",
    "\n",
    "def preprocess_input(x, v2=True):\n",
    "    x = x.astype('float32')\n",
    "    x = x / 255.0\n",
    "    if v2:\n",
    "        x = x - 0.5\n",
    "        x = x * 2.0\n",
    "    return x\n",
    "\n",
    "def draw_bounding_box(face_coordinates, image_array, color):\n",
    "    x, y, w, h = face_coordinates\n",
    "    cv2.rectangle(image_array, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "def draw_text(coordinates, image_array, text, color, x_offset=0, y_offset=0,font_scale=2, thickness=2):\n",
    "    x, y = coordinates[:2]\n",
    "    cv2.putText(image_array, text, (x + x_offset, y + y_offset),cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                font_scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "def apply_offsets(face_coordinates, offsets):\n",
    "    x, y, width, height = face_coordinates\n",
    "    x_off, y_off = offsets\n",
    "    return (x - x_off, x + width + x_off, y - y_off, y + height + y_off)\n",
    "\n",
    "    \n",
    "def show_frame():\n",
    "   \n",
    "    # hyper-parameters for bounding boxes shape\n",
    "    frame_window = 10\n",
    "    emotion_offsets = (20, 40)\n",
    "    # loading models\n",
    "\n",
    "     # getting input model shapes for inference\n",
    "    emotion_target_size = emotion_classifier.input_shape[1:3]\n",
    "\n",
    "    # starting lists for calculating modes\n",
    "    emotion_window = []\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    #cv2.putText(frame,'OpenCV',(10,30), cv2.FONT_HERSHEY_SIMPLEX, 4,(255,255,255),2,cv2.LINE_AA)\n",
    "    cv2image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGBA)\n",
    "    gray_image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5,minSize=(30, 30))#, flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "    for face_coordinates in faces:\n",
    "\n",
    "        x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n",
    "\n",
    "        gray_face = gray_image[y1:y2, x1:x2]\n",
    "        try:\n",
    "            gray_face = cv2.resize(gray_face, (emotion_target_size))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        gray_face = preprocess_input(gray_face, True)\n",
    "        gray_face = np.expand_dims(gray_face, 0)\n",
    "        gray_face = np.expand_dims(gray_face, -1)\n",
    "        emotion_prediction = emotion_classifier.predict(gray_face)\n",
    "        emotion_probability = np.max(emotion_prediction)\n",
    "        emotion_label_arg = np.argmax(emotion_prediction)\n",
    "        emotion_text = emotion_labels[emotion_label_arg]\n",
    "        emotion_window.append(emotion_text)\n",
    "        \n",
    "        if len(emotion_window) > frame_window:\n",
    "            emotion_window.pop(0)\n",
    "        try:\n",
    "            emotion_mode = mode(emotion_window)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if emotion_text == 'angry':\n",
    "            color = emotion_probability * np.asarray((255, 0, 0))\n",
    "        elif emotion_text == 'sad':\n",
    "            color = emotion_probability * np.asarray((0, 0, 255))\n",
    "        elif emotion_text == 'happy':\n",
    "            color = emotion_probability * np.asarray((255, 255, 0))\n",
    "        elif emotion_text == 'surprise':\n",
    "            color = emotion_probability * np.asarray((0, 255, 255))\n",
    "        else:\n",
    "            color = emotion_probability * np.asarray((0, 255, 0))\n",
    "\n",
    "        color = color.astype(int)\n",
    "        color = color.tolist()\n",
    "        \n",
    "        mood['text']=emotion_text\n",
    "        x, y = face_coordinates[:2]\n",
    "        cv2.putText(frame, emotion_text, (50 , 50 ),cv2.FONT_HERSHEY_SIMPLEX, 4,(255,255,255),2,cv2.LINE_AA) \n",
    "        draw_bounding_box(face_coordinates, frame, color)\n",
    "       \n",
    "    bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
    "    img = PIL.Image.fromarray(cv2image)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "    \n",
    "    lmain.imgtk = imgtk\n",
    "    lmain.configure(image=imgtk)\n",
    "    lmain.after(10, show_frame)\n",
    "    \n",
    "\n",
    "import PIL\n",
    "from PIL import Image,ImageTk\n",
    "import pytesseract\n",
    "import cv2\n",
    "from tkinter import *\n",
    "import numpy as np \n",
    "import cv2\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from statistics import mode\n",
    "\n",
    "    # parameters for loading data and images\n",
    "emotion_model_path = 'emotion_model.hdf5'\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "emotion_classifier = load_model(emotion_model_path)\n",
    "   \n",
    "emotion_labels = {0:'angry',1:'disgust',2:'fear',3:'happy',4:'sad',5:'surprise',6:'neutral'}\n",
    "width, height = 800, 600\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "root = Tk()\n",
    "f1 = Frame(root)\n",
    "f2 = Frame(root)\n",
    "for frame in (f1, f2):\n",
    "    frame.grid(row=0, column=0, sticky='news')\n",
    "\n",
    "lmain = Label(f1)\n",
    "lmain.pack()\n",
    "Button(f1, text='webcam',command=show_frame).pack()\n",
    "Button(f1, text='close',command=root.destroy).pack()\n",
    "Button(f1, text='page 2', command=lambda:raise_frame(f2)).pack()\n",
    "Label(f1, text='FRAME 1').pack()\n",
    "mood= Label(f1, text=\"None\")\n",
    "mood.pack()\n",
    "\n",
    "Button(f2, text='close',command=root.destroy).pack()\n",
    "Button(f2, text='page 2', command=lambda:raise_frame(f1)).pack()\n",
    "Label(f2, text='FRAME 1').pack()\n",
    "\n",
    "raise_frame(f1)\n",
    "root.mainloop()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of C Empty\n"
     ]
    }
   ],
   "source": [
    "def raise_frame(frame):\n",
    "    frame.tkraise()\n",
    "    \n",
    "def song():\n",
    "    print(\"Value of C\",C)\n",
    "    if C==\"angry\":\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "    elif C==\"happy\":\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "    elif C==\"sad\":\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "    elif C==\"suprise\":\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "    else:\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "        \n",
    "\n",
    "def video():\n",
    "    print(\"Value of C\",C)\n",
    "    if C==\"angry\":\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "    elif C==\"happy\":\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "    elif C==\"sad\":\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "    elif C==\"suprise\":\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "    else:\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "\n",
    "def coffee():\n",
    "    webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "\n",
    "def preprocess_input(x, v2=True):\n",
    "    x = x.astype('float32')\n",
    "    x = x / 255.0\n",
    "    if v2:\n",
    "        x = x - 0.5\n",
    "        x = x * 2.0\n",
    "    return x\n",
    "\n",
    "def apply_offsets(face_coordinates, offsets):\n",
    "    x, y, width, height = face_coordinates\n",
    "    x_off, y_off = offsets\n",
    "    return (x - x_off, x + width + x_off, y - y_off, y + height + y_off)\n",
    "\n",
    "    \n",
    "def show_frame():\n",
    "   \n",
    "    # hyper-parameters for bounding boxes shape\n",
    "    frame_window = 10\n",
    "    emotion_offsets = (20, 40)\n",
    "    # loading models\n",
    "\n",
    "     # getting input model shapes for inference\n",
    "    emotion_target_size = emotion_classifier.input_shape[1:3]\n",
    "\n",
    "    # starting lists for calculating modes\n",
    "    emotion_window = []\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    #cv2.putText(frame,'OpenCV',(10,30), cv2.FONT_HERSHEY_SIMPLEX, 4,(255,255,255),2,cv2.LINE_AA)\n",
    "    cv2image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGBA)\n",
    "    gray_image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5,minSize=(30, 30))#, flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "    for face_coordinates in faces:\n",
    "\n",
    "        x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n",
    "\n",
    "        gray_face = gray_image[y1:y2, x1:x2]\n",
    "        try:\n",
    "            gray_face = cv2.resize(gray_face, (emotion_target_size))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        gray_face = preprocess_input(gray_face, True)\n",
    "        gray_face = np.expand_dims(gray_face, 0)\n",
    "        gray_face = np.expand_dims(gray_face, -1)\n",
    "        emotion_prediction = emotion_classifier.predict(gray_face)\n",
    "        emotion_probability = np.max(emotion_prediction)\n",
    "        emotion_label_arg = np.argmax(emotion_prediction)\n",
    "        emotion_text = emotion_labels[emotion_label_arg]\n",
    "        emotion_window.append(emotion_text)\n",
    "\n",
    "        if emotion_text == 'angry':\n",
    "            color = \"red\"\n",
    "        elif emotion_text == 'sad':\n",
    "            color = \"blue\"\n",
    "        elif emotion_text == 'happy':\n",
    "            color = \"yellow\"\n",
    "        elif emotion_text == 'surprise':\n",
    "            color = \"orange\"\n",
    "        else:\n",
    "            color = \"green\"\n",
    "\n",
    "        mood['text']=emotion_text\n",
    "        mood['bg']=color\n",
    "        mood1['text']=emotion_text\n",
    "        mood1['bg']=color\n",
    "        global C\n",
    "        C=emotion_text\n",
    "       \n",
    "    bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
    "    img = PIL.Image.fromarray(cv2image)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "    \n",
    "    lmain.imgtk = imgtk\n",
    "    lmain.configure(image=imgtk)\n",
    "    lmain.after(10, show_frame)\n",
    "    \n",
    "    \n",
    "import webbrowser\n",
    "import PIL\n",
    "from PIL import Image,ImageTk\n",
    "import pytesseract\n",
    "import cv2\n",
    "from tkinter import *\n",
    "import numpy as np \n",
    "import cv2\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from statistics import mode\n",
    "global emotion_text\n",
    "    # parameters for loading data and images\n",
    "emotion_model_path = 'emotion_model.hdf5'\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "emotion_classifier = load_model(emotion_model_path)\n",
    "   \n",
    "emotion_labels = {0:'angry',1:'disgust',2:'fear',3:'happy',4:'sad',5:'surprise',6:'neutral'}\n",
    "width, height = 800, 600\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "C=\"Empty\"\n",
    "root = Tk()\n",
    "f1 = Frame(root)\n",
    "f2 = Frame(root)\n",
    "for frame in (f1, f2):\n",
    "    frame.grid(row=0, column=0, sticky='news')\n",
    "\n",
    "lmain = Label(f1)\n",
    "lmain.pack()\n",
    "\n",
    "Label(f1, text='FRAME 1').pack(side=TOP)\n",
    "Button(f1, text='webcam',command=show_frame).pack()\n",
    "Button(f1, text='Close',command=root.destroy).pack()\n",
    "Button(f1, text='Go to menu', command=lambda:raise_frame(f2)).pack()\n",
    "Label(f1, text='MOOD : ').pack(side=LEFT)\n",
    "\n",
    "mood= Label(f1, text=\"None\",bg=\"white\")\n",
    "mood.pack(side=LEFT)\n",
    "\n",
    "Label(f2, text='FRAME 2').pack()\n",
    "Label(f2, text='MOOD : ').pack(side=TOP,fill=X)\n",
    "mood1= Label(f2, text=\"None\",bg=\"white\")\n",
    "mood1.pack(side=TOP,fill=X)\n",
    "\n",
    "Button(f2, text='Go to webcam', command=lambda:raise_frame(f1)).pack()\n",
    "Button(f2, text='Song', command=song).pack()\n",
    "Button(f2, text='Video', command=video).pack()\n",
    "Button(f2, text='Coffee', command=coffee).pack()\n",
    "Button(f2, text='Close',command=root.destroy).pack()\n",
    "\n",
    "raise_frame(f1)\n",
    "root.mainloop()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v5 color and layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raise_frame(frame):\n",
    "    frame.tkraise()\n",
    "    \n",
    "def song():\n",
    "    if C==\"angry\":\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "    elif C==\"happy\":\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "    elif C==\"sad\":\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "    elif C==\"suprise\":\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "    else:\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "\n",
    "def video():\n",
    "    if C==\"angry\":\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "    elif C==\"happy\":\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "    elif C==\"sad\":\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "    elif C==\"suprise\":\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "    else:\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "\n",
    "def coffee():\n",
    "    if C==\"angry\":\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "    elif C==\"happy\":\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "    elif C==\"sad\":\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "    elif C==\"suprise\":\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "    else:\n",
    "        webbrowser.open_new_tab(\"https://soundcloud.com/\")\n",
    "\n",
    "def preprocess_input(x, v2=True):\n",
    "    x = x.astype('float32')\n",
    "    x = x / 255.0\n",
    "    if v2:\n",
    "        x = x - 0.5\n",
    "        x = x * 2.0\n",
    "    return x\n",
    "\n",
    "def apply_offsets(face_coordinates, offsets):\n",
    "    x, y, width, height = face_coordinates\n",
    "    x_off, y_off = offsets\n",
    "    return (x - x_off, x + width + x_off, y - y_off, y + height + y_off)\n",
    "\n",
    "    \n",
    "def show_frame():\n",
    "   \n",
    "    # hyper-parameters for bounding boxes shape\n",
    "    frame_window = 10\n",
    "    emotion_offsets = (20, 40)\n",
    "    # loading models\n",
    "\n",
    "     # getting input model shapes for inference\n",
    "    emotion_target_size = emotion_classifier.input_shape[1:3]\n",
    "\n",
    "    # starting lists for calculating modes\n",
    "    emotion_window = []\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    gray_image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5,minSize=(30, 30))\n",
    "    \n",
    "    cv2image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGBA)\n",
    "    \n",
    "    rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5,minSize=(30, 30))\n",
    "    \n",
    "    for face_coordinates in faces:\n",
    "\n",
    "        x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n",
    "\n",
    "        gray_face = gray_image[y1:y2, x1:x2]\n",
    "        try:\n",
    "            gray_face = cv2.resize(gray_face, (emotion_target_size))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        gray_face = preprocess_input(gray_face, True)\n",
    "        gray_face = np.expand_dims(gray_face, 0)\n",
    "        gray_face = np.expand_dims(gray_face, -1)\n",
    "        emotion_prediction = emotion_classifier.predict(gray_face)\n",
    "        emotion_probability = np.max(emotion_prediction)\n",
    "        emotion_label_arg = np.argmax(emotion_prediction)\n",
    "        emotion_text = emotion_labels[emotion_label_arg]\n",
    "        emotion_window.append(emotion_text)\n",
    "\n",
    "        if emotion_text == 'angry':\n",
    "            style.configure(\"BW.TLabel\", foreground=\"black\", background=\"red\")\n",
    "        elif emotion_text == 'sad':\n",
    "            style.configure(\"BW.TLabel\", foreground=\"black\", background=\"blue\")\n",
    "        elif emotion_text == 'happy':\n",
    "            style.configure(\"BW.TLabel\", foreground=\"black\", background=\"yellow\")\n",
    "        elif emotion_text == 'surprise':\n",
    "            style.configure(\"BW.TLabel\", foreground=\"black\", background=\"orange\")\n",
    "        else:\n",
    "            style.configure(\"BW.TLabel\", foreground=\"black\", background=\"green\")\n",
    "\n",
    "        mood['text']=emotion_text\n",
    "        mood['style']=\"BW.TLabel\"\n",
    "        mood1['text']=emotion_text\n",
    "        mood1['style']=\"BW.TLabel\"\n",
    "        global C\n",
    "        C=emotion_text\n",
    "        cv2.putText(cv2image, emotion_text, (face_coordinates[0]+20, face_coordinates[1]-20), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    2.0, (0, 0, 255),3)\n",
    "        cv2.rectangle(cv2image, (face_coordinates[0], face_coordinates[1]),\n",
    "                      (face_coordinates[0]+face_coordinates[3], face_coordinates[1]+face_coordinates[3]), (255, 0, 0), 4)\n",
    "       \n",
    "    bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
    "    img = PIL.Image.fromarray(cv2image)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "    \n",
    "    lmain.imgtk = imgtk\n",
    "    lmain.configure(image=imgtk)\n",
    "    lmain.after(10, show_frame)\n",
    "    \n",
    "    \n",
    "import webbrowser\n",
    "import PIL\n",
    "from PIL import Image,ImageTk\n",
    "import pytesseract\n",
    "import cv2\n",
    "from tkinter import *\n",
    "from tkinter.ttk import *\n",
    "import numpy as np \n",
    "import cv2\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from statistics import mode\n",
    "global emotion_text\n",
    "    # parameters for loading data and images\n",
    "emotion_model_path = 'emotion_model.hdf5'\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "emotion_classifier = load_model(emotion_model_path)\n",
    "   \n",
    "emotion_labels = {0:'angry',1:'disgust',2:'fear',3:'happy',4:'sad',5:'surprise',6:'neutral'}\n",
    "width, height = 800, 600\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "C=\"Empty\"\n",
    "root = Tk()\n",
    "f1 = Frame(root)\n",
    "f2 = Frame(root)\n",
    "for frame in (f1, f2):\n",
    "    frame.grid(row=0, column=0, sticky='news')\n",
    "style =Style()\n",
    "style.configure(\"BW.TLabel\", foreground=\"black\", background=\"white\")\n",
    "\n",
    "lmain = Label(f1)\n",
    "lmain.pack()\n",
    "\n",
    "Label(f1, text='FRAME 1').pack(side=TOP)\n",
    "Button(f1, text='webcam',command=show_frame).pack()\n",
    "Button(f1, text='Close',command=root.destroy).pack()\n",
    "Button(f1, text='Go to menu', command=lambda:raise_frame(f2)).pack()\n",
    "Label(f1, text='MOOD : ').pack(side=LEFT)\n",
    "\n",
    "mood= Label(f1, text=\"None\",style=\"BW.TLabel\")\n",
    "mood.pack(side=LEFT)\n",
    "\n",
    "Label(f2, text='FRAME 2').pack()\n",
    "Label(f2, text='MOOD : ').pack(side=TOP,fill=X)\n",
    "mood1= Label(f2, text=\"None\",style=\"BW.TLabel\")\n",
    "mood1.pack(side=TOP,fill=X)\n",
    "\n",
    "Button(f2, text='Go to webcam', command=lambda:raise_frame(f1)).pack()\n",
    "Button(f2, text='Song', command=song).pack()\n",
    "Button(f2, text='Video', command=video).pack()\n",
    "Button(f2, text='Coffee', command=coffee).pack()\n",
    "Button(f2, text='Close',command=root.destroy).pack()\n",
    "\n",
    "raise_frame(f1)\n",
    "root.mainloop()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
